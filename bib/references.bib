
@article{fonseca_residual_2022,
	title = {A Residual Movement Classification Based User Interface for Control of Assistive Devices by Persons With Complete Tetraplegia},
	volume = {30},
	rights = {https://creativecommons.org/licenses/by/4.0/legalcode},
	issn = {1534-4320, 1558-0210},
	url = {https://ieeexplore.ieee.org/document/9725807/},
	doi = {10.1109/TNSRE.2022.3156269},
	pages = {569--578},
	journaltitle = {{IEEE} Transactions on Neural Systems and Rehabilitation Engineering},
	shortjournal = {{IEEE} Trans. Neural Syst. Rehabil. Eng.},
	author = {Fonseca, Lucas and Guiraud, David and Hiairrassary, Arthur and Fattal, Charles and Azevedo-Coste, Christine},
	urldate = {2025-05-13},
	year = {2022},
	type = {journal article},
	series = {IEEE Transactions on Neural Systems and Rehabilitation Engineering},
	file = {Full Text:C\:\\Users\\User\\Zotero\\storage\\FHZ4U6XG\\Fonseca et al. - 2022 - A Residual Movement Classification Based User Inte.pdf:application/pdf},
}

@article{lee_learning_2024,
	title = {Learning to Control Complex Robots Using High-Dimensional Body-Machine Interfaces},
	volume = {13},
	issn = {2573-9522},
	url = {https://dl.acm.org/doi/10.1145/3630264},
	doi = {10.1145/3630264},
	abstract = {When individuals are paralyzed from injury or damage to the brain, upper body movement and function can be compromised. While the use of body motions to interface with machines has shown to be an effective noninvasive strategy to provide movement assistance and to promote physical rehabilitation, learning to use such interfaces to control complex machines is not well understood. In a five session study, we demonstrate that a subset of an uninjured population is able to learn and improve their ability to use a high-dimensional Body-Machine Interface ({BoMI}), to control a robotic arm. We use a sensor net of four inertial measurement units, placed bilaterally on the upper body, and a {BoMI} with the capacity to directly control a robot in six dimensions. We consider whether the way in which the robot control space is mapped from human inputs has any impact on learning. Our results suggest that the space of robot control does play a role in the evolution of human learning: specifically, though robot control in joint space appears to be more intuitive initially, control in task space is found to have a greater capacity for longer-term improvement and learning. Our results further suggest that there is an inverse relationship between control dimension couplings and task performance.},
	pages = {1--20},
	number = {3},
	journaltitle = {{ACM} Transactions on Human-Robot Interaction},
	shortjournal = {J. Hum.-Robot Interact.},
	author = {Lee, Jongmin and Gebrekristos, Temesgen and De Santis, Dalia and Nejati-Javaremi, Mahdieh and Gopinath, Deepak and Parikh, Biraj and Mussa-Ivaldi, Ferdinando and Argall, Brenna},
	urldate = {2025-05-13},
	year = {2024-09-30},
	langid = {english},
	type = {conference paper},
	series = {IEEE International Conference on Rehabilitation Robotics (ICORR)},
	file = {Full Text:C\:\\Users\\User\\Zotero\\storage\\ETZTRIME\\Lee et al. - 2024 - Learning to Control Complex Robots Using High-Dime.pdf:application/pdf},
}

@article{gantenbein_intention_2022,
	title = {Intention Detection Strategies for Robotic Upper-Limb Orthoses: A Scoping Review Considering Usability, Daily Life Application, and User Evaluation},
	volume = {16},
	issn = {1662-5218},
	url = {https://www.frontiersin.org/articles/10.3389/fnbot.2022.815693/full},
	doi = {10.3389/fnbot.2022.815693},
	shorttitle = {Intention Detection Strategies for Robotic Upper-Limb Orthoses},
	abstract = {Wearable robotic upper limb orthoses ({ULO}) are promising tools to assist or enhance the upper-limb function of their users. While the functionality of these devices has continuously increased, the robust and reliable detection of the user's intention to control the available degrees of freedom remains a major challenge and a barrier for acceptance. As the information interface between device and user, the intention detection strategy ({IDS}) has a crucial impact on the usability of the overall device. Yet, this aspect and the impact it has on the device usability is only rarely evaluated with respect to the context of use of {ULO}. A scoping literature review was conducted to identify non-invasive {IDS} applied to {ULO} that have been evaluated with human participants, with a specific focus on evaluation methods and findings related to functionality and usability and their appropriateness for specific contexts of use in daily life. A total of 93 studies were identified, describing 29 different {IDS} that are summarized and classified according to a four-level classification scheme. The predominant user input signal associated with the described {IDS} was electromyography (35.6\%), followed by manual triggers such as buttons, touchscreens or joysticks (16.7\%), as well as isometric force generated by residual movement in upper-limb segments (15.1\%). We identify and discuss the strengths and weaknesses of {IDS} with respect to specific contexts of use and highlight a trade-off between performance and complexity in selecting an optimal {IDS}. Investigating evaluation practices to study the usability of {IDS}, the included studies revealed that, primarily, objective and quantitative usability attributes related to effectiveness or efficiency were assessed. Further, it underlined the lack of a systematic way to determine whether the usability of an {IDS} is sufficiently high to be appropriate for use in daily life applications. This work highlights the importance of a user- and application-specific selection and evaluation of non-invasive {IDS} for {ULO}. For technology developers in the field, it further provides recommendations on the selection process of {IDS} as well as to the design of corresponding evaluation protocols.},
	pages = {815693},
	journaltitle = {Frontiers in Neurorobotics},
	shortjournal = {Front. Neurorobot.},
	author = {Gantenbein, Jessica and Dittli, Jan and Meyer, Jan Thomas and Gassert, Roger and Lambercy, Olivier},
	urldate = {2025-05-13},
	year = {2022-02-21},
	series = {Frontiers in Neurorobotics},
	type = {journal article},
	file = {Full Text:C\:\\Users\\User\\Zotero\\storage\\J722YH8R\\Gantenbein et al. - 2022 - Intention Detection Strategies for Robotic Upper-L.pdf:application/pdf},
}

@article{10.1145/3494994,
author = {Bhalla, Sejal and Goel, Mayank and Khurana, Rushil},
title = {IMU2Doppler: Cross-Modal Domain Adaptation for Doppler-based Activity Recognition Using IMU Data},
year = {2022},
issue_date = {Dec 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {4},
url = {https://doi.org/10.1145/3494994},
doi = {10.1145/3494994},
abstract = {The proliferation of sensors powered by state-of-the-art machine learning techniques can now infer context, recognize activities and enable interactions. A key component required to build these automated sensing systems is labeled training data. However, the cost of collecting and labeling new data impedes our ability to deploy new sensors to recognize human activities. We tackle this challenge using domain adaptation i.e., using existing labeled data in a different domain to aid the training of a machine learning model for a new sensor. In this paper, we use off-the-shelf smartwatch IMU datasets to train an activity recognition system for mmWave radar sensor with minimally labeled data. We demonstrate that despite the lack of extensive datasets for mmWave radar, we are able to use our domain adaptation approach to build an activity recognition system that classifies between 10 activities with an accuracy of 70\% with only 15 seconds of labeled doppler data. We also present results for a range of available labeled data (10 - 30 seconds) and show that our approach outperforms the baseline in every single scenario. We take our approach a step further and show that multiple IMU datasets can be combined together to act as a single source for our domain adaptation approach. Lastly, we discuss the limitations of our work and how it can impact future research directions.},
journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
month = dec,
articleno = {145},
numpages = {20},
type = {journal article},
series = {Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies (IMWUT)},
keywords = {doppler sensor, domain adaptation}
}

@article{koval_experimental_2022,
	title = {Experimental evaluation of autonomous map-based Spot navigation in confined environments},
	volume = {2},
	issn = {26673797},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S2667379722000018},
	doi = {10.1016/j.birob.2022.100035},
	pages = {100035},
	number = {1},
	journaltitle = {Biomimetic Intelligence and Robotics},
	shortjournal = {Biomimetic Intelligence and Robotics},
	author = {Koval, Anton and Karlsson, Samuel and Nikolakopoulos, George},
	urldate = {2025-05-15},
	year = {2022-03},
	langid = {english},
	type = {conference paper},
	series = {Biomimetic Intelligence and Robotics},
	file = {Full Text:C\:\\Users\\User\\Zotero\\storage\\RKSLBZXR\\Koval et al. - 2022 - Experimental evaluation of autonomous map-based Sp.pdf:application/pdf},
}

@article{lee_electromyogram-based_2021,
	title = {Electromyogram-Based Classification of Hand and Finger Gestures Using Artificial Neural Networks},
	volume = {22},
	issn = {1424-8220},
	doi = {10.3390/s22010225},
	abstract = {Electromyogram ({EMG}) signals have been increasingly used for hand and finger gesture recognition. However, most studies have focused on the wrist and whole-hand gestures and not on individual finger ({IF}) gestures, which are considered more challenging. In this study, we develop {EMG}-based hand/finger gesture classifiers based on fixed electrode placement using machine learning methods. Ten healthy subjects performed ten hand/finger gestures, including seven {IF} gestures. {EMG} signals were measured from three channels, and six time-domain ({TD}) features were extracted from each channel. A total of 18 features was used to build personalized classifiers for ten gestures with an artificial neural network ({ANN}), a support vector machine ({SVM}), a random forest ({RF}), and a logistic regression ({LR}). The {ANN}, {SVM}, {RF}, and {LR} achieved mean accuracies of 0.940, 0.876, 0.831, and 0.539, respectively. One-way analyses of variance and F-tests showed that the {ANN} achieved the highest mean accuracy and the lowest inter-subject variance in the accuracy, respectively, suggesting that it was the least affected by individual variability in {EMG} signals. Using only {TD} features, we achieved a higher ratio of gestures to channels than other similar studies, suggesting that the proposed method can improve the system usability and reduce the computational burden.},
	pages = {225},
	number = {1},
	journaltitle = {Sensors (Basel, Switzerland)},
	shortjournal = {Sensors (Basel)},
	author = {Lee, Kyung Hyun and Min, Ji Young and Byun, Sangwon},
	year = {2021-12-29},
	pmid = {35009768},
	pmcid = {PMC8749583},
	series = {Sensors (Basel)},
	type = {journal article},
	keywords = {classification, Algorithms, artificial neural network, electromyogram, Electromyography, {EMG}, gesture recognition, Gestures, Hand, hand-finger movement, Humans, machine learning, Machine Learning, Neural Networks, Computer, physiological signal, prosthetic hand, time-domain features},
	file = {Full Text:C\:\\Users\\User\\Zotero\\storage\\YLKHDNQM\\Lee et al. - 2021 - Electromyogram-Based Classification of Hand and Fi.pdf:application/pdf},
}

@article{repnik_using_2018,
	title = {Using Inertial Measurement Units and Electromyography to Quantify Movement during Action Research Arm Test Execution},
	volume = {18},
	issn = {1424-8220},
	doi = {10.3390/s18092767},
	abstract = {In patients after stroke, ability of the upper limb is commonly assessed with standardised clinical tests that provide a complete upper limb assessment. This paper presents quantification of upper limb movement during the execution of Action research arm test ({ARAT}) using a wearable system of inertial measurement units ({IMU}) for kinematic quantification and electromyography ({EMG}) sensors for muscle activity analysis. The test was executed with each arm by a group of healthy subjects and a group of patients after stroke allocated into subgroups based on their clinical scores. Tasks were segmented into movement and manipulation phases. Each movement phase was quantified with a set of five parameters: movement time, movement smoothness, hand trajectory similarity, trunk stability, and muscle activity for grasping. Parameters vary between subject groups, between tasks, and between task phases. Statistically significant differences were observed between patient groups that obtained different clinical scores, between healthy subjects and patients, and between the unaffected and the affected arm unless the affected arm shows normal performance. Movement quantification enables differentiation between different subject groups within movement phases as well as for the complete task. Spearman's rank correlation coefficient shows strong correlations between patient's {ARAT} scores and movement time as well as movement smoothness. Weak to moderate correlations were observed for parameters that describe hand trajectory similarity and trunk stability. Muscle activity correlates well with grasping activity and the level of grasping force in all groups.},
	pages = {2767},
	number = {9},
	journaltitle = {Sensors (Basel, Switzerland)},
	shortjournal = {Sensors (Basel)},
	author = {Repnik, Eva and Puh, Urška and Goljar, Nika and Munih, Marko and Mihelj, Matjaž},
	year = {2018-08-22},
	pmid = {30135413},
	pmcid = {PMC6164634},
	series = {Sensors (Basel)},
	type = {journal article},
	keywords = {Electromyography, Humans, Adult, Aged, {ARAT}, Arm, Biomechanical Phenomena, Case-Control Studies, electromyography, Female, inertial measurement unit, Male, Middle Aged, Movement, quantification, stroke, Stroke, upper-limb movement},
	file = {Full Text:C\:\\Users\\User\\Zotero\\storage\\568Z7HUM\\Repnik et al. - 2018 - Using Inertial Measurement Units and Electromyogra.pdf:application/pdf},
}

@article{kundu_hand_2018,
	title = {Hand Gesture Recognition Based Omnidirectional Wheelchair Control Using {IMU} and {EMG} Sensors},
	volume = {91},
	issn = {0921-0296, 1573-0409},
	url = {http://link.springer.com/10.1007/s10846-017-0725-0},
	doi = {10.1007/s10846-017-0725-0},
	pages = {529--541},
	number = {3},
	journaltitle = {Journal of Intelligent \& Robotic Systems},
	shortjournal = {J Intell Robot Syst},
	author = {Kundu, Ananda Sankar and Mazumder, Oishee and Lenka, Prasanna Kumar and Bhaumik, Subhasis},
	urldate = {2025-05-15},
	year = {2018-09},
	type = {journal article},
	langid = {english},
}

@article{krasoulis_multi-grip_2020,
	title = {Multi-Grip Classification-Based Prosthesis Control With Two {EMG}-{IMU} Sensors},
	volume = {28},
	rights = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/{IEEE}.html},
	issn = {1534-4320, 1558-0210},
	url = {https://ieeexplore.ieee.org/document/8932579/},
	doi = {10.1109/TNSRE.2019.2959243},
	pages = {508--518},
	number = {2},
	journaltitle = {{IEEE} Transactions on Neural Systems and Rehabilitation Engineering},
	shortjournal = {{IEEE} Trans. Neural Syst. Rehabil. Eng.},
	author = {Krasoulis, Agamemnon and Vijayakumar, Sethu and Nazarpour, Kianoush},
	urldate = {2025-05-15},
	year = {2020-02},
	type = {journal article},
	series = {Journal of Intelligent & Robotic Systems},
	file = {Accepted Version:C\:\\Users\\User\\Zotero\\storage\\TT7RWC3S\\Krasoulis et al. - 2020 - Multi-Grip Classification-Based Prosthesis Control.pdf:application/pdf},
}

@article{zhang_walk_2025,
	title = {Walk Along: An Experiment on Controlling the Mobile Robot ‘Spot’ with Voice and Gestures},
	issn = {2573-9522},
	url = {https://dl.acm.org/doi/10.1145/3729540},
	doi = {10.1145/3729540},
	shorttitle = {Walk Along},
	abstract = {Robots are becoming more capable and can autonomously perform tasks such as navigating between locations. However, human oversight remains crucial. This study compared two touchless methods for directing mobile robots: voice control and gesture control, to investigate the efficiency of these methods and the preference of users. We tested these methods in two conditions: one in which participants remained stationary and one in which they walked freely alongside the robot. We hypothesized that walking alongside the robot would result in higher intuitiveness ratings and improved task performance, based on the idea that walking promotes spatial alignment and reduces the effort required for mental rotation. In a 2×2 within-subject design, 218 participants guided the quadruped robot Spot along a circuitous route with multiple 90° turns using rotate left, rotate right, and walk forward commands. After each trial, participants rated the intuitiveness of the command mapping, while post-experiment interviews were used to gather the participants’ preferences. Results showed that voice control combined with walking with Spot was the most favored and intuitive, whereas gesture control while standing caused confusion for left/right commands. Nevertheless, 29\% of participants preferred gesture control, citing increased task engagement and visual congruence as reasons. An odometry-based analysis revealed that participants often followed behind Spot, particularly in the gesture control condition, when they were allowed to walk. In conclusion, voice control with walking produced the best outcomes. Improving physical ergonomics and adjusting gesture types could make gesture control more effective.},
	pages = {3729540},
	journaltitle = {{ACM} Transactions on Human-Robot Interaction},
	shortjournal = {J. Hum.-Robot Interact.},
	author = {Zhang, Renchi and Van Der Linden, Jesse and Dodou, Dimitra and Seyffert, Harleigh and Eisma, Yke Bauke and De Winter, Joost},
	urldate = {2025-05-15},
	year = {2025-04-12},
	type = {journal article},
	series = {ACM Transactions on Human-Robot Interaction},
	langid = {english},
}
