define({ entries : {
    "10.1145/3494994": {
        "abstract": "The proliferation of sensors powered by state-of-the-art machine learning techniques can now infer context, recognize activities and enable interactions. A key component required to build these automated sensing systems is labeled training data. However, the cost of collecting and labeling new data impedes our ability to deploy new sensors to recognize human activities. We tackle this challenge using domain adaptation i.e., using existing labeled data in a different domain to aid the training of a machine learning model for a new sensor. In this paper, we use off-the-shelf smartwatch IMU datasets to train an activity recognition system for mmWave radar sensor with minimally labeled data. We demonstrate that despite the lack of extensive datasets for mmWave radar, we are able to use our domain adaptation approach to build an activity recognition system that classifies between 10 activities with an accuracy of 70\\% with only 15 seconds of labeled doppler data. We also present results for a range of available labeled data (10 - 30 seconds) and show that our approach outperforms the baseline in every single scenario. We take our approach a step further and show that multiple IMU datasets can be combined together to act as a single source for our domain adaptation approach. Lastly, we discuss the limitations of our work and how it can impact future research directions.",
        "address": "New York, NY, USA",
        "articleno": "145",
        "author": "Bhalla, Sejal and Goel, Mayank and Khurana, Rushil",
        "doi": "10.1145/3494994",
        "issue_date": "Dec 2021",
        "journal": "Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.",
        "keywords": "doppler sensor, domain adaptation",
        "month": "dec,",
        "number": "4",
        "numpages": "20",
        "publisher": "Association for Computing Machinery",
        "series": "Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies (IMWUT)",
        "title": "IMU2Doppler: Cross-Modal Domain Adaptation for Doppler-based Activity Recognition Using IMU Data",
        "type": "article journal article",
        "url": "https://doi.org/10.1145/3494994",
        "volume": "5",
        "year": "2022"
    },
    "fonseca_residual_2022": {
        "author": "Fonseca, Lucas and Guiraud, David and Hiairrassary, Arthur and Fattal, Charles and Azevedo-Coste, Christine",
        "doi": "10.1109/TNSRE.2022.3156269",
        "file": "Full Text:C\\:\\\\Users\\\\User\\\\Zotero\\\\storage\\\\FHZ4U6XG\\\\Fonseca et al. - 2022 - A Residual Movement Classification Based User Inte.pdf:application/pdf",
        "issn": "1534-4320, 1558-0210",
        "journaltitle": "{IEEE} Transactions on Neural Systems and Rehabilitation Engineering",
        "pages": "569--578",
        "rights": "https://creativecommons.org/licenses/by/4.0/legalcode",
        "series": "IEEE Transactions on Neural Systems and Rehabilitation Engineering",
        "shortjournal": "{IEEE} Trans. Neural Syst. Rehabil. Eng.",
        "title": "A Residual Movement Classification Based User Interface for Control of Assistive Devices by Persons With Complete Tetraplegia",
        "type": "article journal article",
        "url": "https://ieeexplore.ieee.org/document/9725807/",
        "urldate": "2025-05-13",
        "volume": "30",
        "year": "2022"
    },
    "gantenbein_intention_2022": {
        "abstract": "Wearable robotic upper limb orthoses ({ULO}) are promising tools to assist or enhance the upper-limb function of their users. While the functionality of these devices has continuously increased, the robust and reliable detection of the user's intention to control the available degrees of freedom remains a major challenge and a barrier for acceptance. As the information interface between device and user, the intention detection strategy ({IDS}) has a crucial impact on the usability of the overall device. Yet, this aspect and the impact it has on the device usability is only rarely evaluated with respect to the context of use of {ULO}. A scoping literature review was conducted to identify non-invasive {IDS} applied to {ULO} that have been evaluated with human participants, with a specific focus on evaluation methods and findings related to functionality and usability and their appropriateness for specific contexts of use in daily life. A total of 93 studies were identified, describing 29 different {IDS} that are summarized and classified according to a four-level classification scheme. The predominant user input signal associated with the described {IDS} was electromyography (35.6\\%), followed by manual triggers such as buttons, touchscreens or joysticks (16.7\\%), as well as isometric force generated by residual movement in upper-limb segments (15.1\\%). We identify and discuss the strengths and weaknesses of {IDS} with respect to specific contexts of use and highlight a trade-off between performance and complexity in selecting an optimal {IDS}. Investigating evaluation practices to study the usability of {IDS the included studies revealed that, primarily, objective and quantitative usability attributes related to effectiveness or efficiency were assessed. Further, it underlined the lack of a systematic way to determine whether the usability of an {IDS} is sufficiently high to be appropriate for use in daily life applications. This work highlights the importance of a user- and application-specific selection and evaluation of non-invasive {IDS} for {ULO}. For technology developers in the field, it further provides recommendations on the selection process of {IDS} as well as to the design of corresponding evaluation protocols.",
        "author": "Gantenbein, Jessica and Dittli, Jan and Meyer, Jan Thomas and Gassert, Roger and Lambercy, Olivier",
        "doi": "10.3389/fnbot.2022.815693",
        "file": "Full Text:C\\:\\\\Users\\\\User\\\\Zotero\\\\storage\\\\J722YH8R\\\\Gantenbein et al. - 2022 - Intention Detection Strategies for Robotic Upper-L.pdf:application/pdf",
        "issn": "1662-5218",
        "journaltitle": "Frontiers in Neurorobotics",
        "pages": "815693",
        "series": "Frontiers in Neurorobotics",
        "shortjournal": "Front. Neurorobot.",
        "shorttitle": "Intention Detection Strategies for Robotic Upper-Limb Orthoses",
        "title": "Intention Detection Strategies for Robotic Upper-Limb Orthoses: A Scoping Review Considering Usability, Daily Life Application, and User Evaluation",
        "type": "article journal article",
        "url": "https://www.frontiersin.org/articles/10.3389/fnbot.2022.815693/full",
        "urldate": "2025-05-13",
        "volume": "16",
        "year": "2022-02-21"
    },
    "koval_experimental_2022": {
        "author": "Koval, Anton and Karlsson, Samuel and Nikolakopoulos, George",
        "doi": "10.1016/j.birob.2022.100035",
        "file": "Full Text:C\\:\\\\Users\\\\User\\\\Zotero\\\\storage\\\\RKSLBZXR\\\\Koval et al. - 2022 - Experimental evaluation of autonomous map-based Sp.pdf:application/pdf",
        "issn": "26673797",
        "journaltitle": "Biomimetic Intelligence and Robotics",
        "langid": "english",
        "number": "1",
        "pages": "100035",
        "series": "Biomimetic Intelligence and Robotics",
        "shortjournal": "Biomimetic Intelligence and Robotics",
        "title": "Experimental evaluation of autonomous map-based Spot navigation in confined environments",
        "type": "article conference paper",
        "url": "https://linkinghub.elsevier.com/retrieve/pii/S2667379722000018",
        "urldate": "2025-05-15",
        "volume": "2",
        "year": "2022-03"
    },
    "krasoulis_multi-grip_2020": {
        "author": "Krasoulis, Agamemnon and Vijayakumar, Sethu and Nazarpour, Kianoush",
        "doi": "10.1109/TNSRE.2019.2959243",
        "file": "Accepted Version:C\\:\\\\Users\\\\User\\\\Zotero\\\\storage\\\\TT7RWC3S\\\\Krasoulis et al. - 2020 - Multi-Grip Classification-Based Prosthesis Control.pdf:application/pdf",
        "issn": "1534-4320, 1558-0210",
        "journaltitle": "{IEEE} Transactions on Neural Systems and Rehabilitation Engineering",
        "number": "2",
        "pages": "508--518",
        "rights": "https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/{IEEE}.html",
        "series": "Journal of Intelligent & Robotic Systems",
        "shortjournal": "{IEEE} Trans. Neural Syst. Rehabil. Eng.",
        "title": "Multi-Grip Classification-Based Prosthesis Control With Two {EMG}-{IMU} Sensors",
        "type": "article journal article",
        "url": "https://ieeexplore.ieee.org/document/8932579/",
        "urldate": "2025-05-15",
        "volume": "28",
        "year": "2020-02"
    },
    "kundu_hand_2018": {
        "author": "Kundu, Ananda Sankar and Mazumder, Oishee and Lenka, Prasanna Kumar and Bhaumik, Subhasis",
        "doi": "10.1007/s10846-017-0725-0",
        "issn": "0921-0296, 1573-0409",
        "journaltitle": "Journal of Intelligent \\& Robotic Systems",
        "langid": "english",
        "number": "3",
        "pages": "529--541",
        "shortjournal": "J Intell Robot Syst",
        "title": "Hand Gesture Recognition Based Omnidirectional Wheelchair Control Using {IMU} and {EMG} Sensors",
        "type": "article journal article",
        "url": "http://link.springer.com/10.1007/s10846-017-0725-0",
        "urldate": "2025-05-15",
        "volume": "91",
        "year": "2018-09"
    },
    "lee_electromyogram-based_2021": {
        "abstract": "Electromyogram ({EMG}) signals have been increasingly used for hand and finger gesture recognition. However, most studies have focused on the wrist and whole-hand gestures and not on individual finger ({IF}) gestures, which are considered more challenging. In this study, we develop {EMG}-based hand/finger gesture classifiers based on fixed electrode placement using machine learning methods. Ten healthy subjects performed ten hand/finger gestures, including seven {IF} gestures. {EMG} signals were measured from three channels, and six time-domain ({TD}) features were extracted from each channel. A total of 18 features was used to build personalized classifiers for ten gestures with an artificial neural network ({ANN}), a support vector machine ({SVM}), a random forest ({RF}), and a logistic regression ({LR}). The {ANN {SVM {RF and {LR} achieved mean accuracies of 0.940, 0.876, 0.831, and 0.539, respectively. One-way analyses of variance and F-tests showed that the {ANN} achieved the highest mean accuracy and the lowest inter-subject variance in the accuracy, respectively, suggesting that it was the least affected by individual variability in {EMG} signals. Using only {TD} features, we achieved a higher ratio of gestures to channels than other similar studies, suggesting that the proposed method can improve the system usability and reduce the computational burden.",
        "author": "Lee, Kyung Hyun and Min, Ji Young and Byun, Sangwon",
        "doi": "10.3390/s22010225",
        "file": "Full Text:C\\:\\\\Users\\\\User\\\\Zotero\\\\storage\\\\YLKHDNQM\\\\Lee et al. - 2021 - Electromyogram-Based Classification of Hand and Fi.pdf:application/pdf",
        "issn": "1424-8220",
        "journaltitle": "Sensors (Basel, Switzerland)",
        "keywords": "classification, Algorithms, artificial neural network, electromyogram, Electromyography, {EMG gesture recognition, Gestures, Hand, hand-finger movement, Humans, machine learning, Machine Learning, Neural Networks, Computer, physiological signal, prosthetic hand, time-domain features",
        "number": "1",
        "pages": "225",
        "pmcid": "PMC8749583",
        "pmid": "35009768",
        "series": "Sensors (Basel)",
        "shortjournal": "Sensors (Basel)",
        "title": "Electromyogram-Based Classification of Hand and Finger Gestures Using Artificial Neural Networks",
        "type": "article journal article",
        "volume": "22",
        "year": "2021-12-29"
    },
    "lee_learning_2024": {
        "abstract": "When individuals are paralyzed from injury or damage to the brain, upper body movement and function can be compromised. While the use of body motions to interface with machines has shown to be an effective noninvasive strategy to provide movement assistance and to promote physical rehabilitation, learning to use such interfaces to control complex machines is not well understood. In a five session study, we demonstrate that a subset of an uninjured population is able to learn and improve their ability to use a high-dimensional Body-Machine Interface ({BoMI}), to control a robotic arm. We use a sensor net of four inertial measurement units, placed bilaterally on the upper body, and a {BoMI} with the capacity to directly control a robot in six dimensions. We consider whether the way in which the robot control space is mapped from human inputs has any impact on learning. Our results suggest that the space of robot control does play a role in the evolution of human learning: specifically, though robot control in joint space appears to be more intuitive initially, control in task space is found to have a greater capacity for longer-term improvement and learning. Our results further suggest that there is an inverse relationship between control dimension couplings and task performance.",
        "author": "Lee, Jongmin and Gebrekristos, Temesgen and De Santis, Dalia and Nejati-Javaremi, Mahdieh and Gopinath, Deepak and Parikh, Biraj and Mussa-Ivaldi, Ferdinando and Argall, Brenna",
        "doi": "10.1145/3630264",
        "file": "Full Text:C\\:\\\\Users\\\\User\\\\Zotero\\\\storage\\\\ETZTRIME\\\\Lee et al. - 2024 - Learning to Control Complex Robots Using High-Dime.pdf:application/pdf",
        "issn": "2573-9522",
        "journaltitle": "{ACM} Transactions on Human-Robot Interaction",
        "langid": "english",
        "number": "3",
        "pages": "1--20",
        "series": "IEEE International Conference on Rehabilitation Robotics (ICORR)",
        "shortjournal": "J. Hum.-Robot Interact.",
        "title": "Learning to Control Complex Robots Using High-Dimensional Body-Machine Interfaces",
        "type": "article conference paper",
        "url": "https://dl.acm.org/doi/10.1145/3630264",
        "urldate": "2025-05-13",
        "volume": "13",
        "year": "2024-09-30"
    },
    "repnik_using_2018": {
        "abstract": "In patients after stroke, ability of the upper limb is commonly assessed with standardised clinical tests that provide a complete upper limb assessment. This paper presents quantification of upper limb movement during the execution of Action research arm test ({ARAT}) using a wearable system of inertial measurement units ({IMU}) for kinematic quantification and electromyography ({EMG}) sensors for muscle activity analysis. The test was executed with each arm by a group of healthy subjects and a group of patients after stroke allocated into subgroups based on their clinical scores. Tasks were segmented into movement and manipulation phases. Each movement phase was quantified with a set of five parameters: movement time, movement smoothness, hand trajectory similarity, trunk stability, and muscle activity for grasping. Parameters vary between subject groups, between tasks, and between task phases. Statistically significant differences were observed between patient groups that obtained different clinical scores, between healthy subjects and patients, and between the unaffected and the affected arm unless the affected arm shows normal performance. Movement quantification enables differentiation between different subject groups within movement phases as well as for the complete task. Spearman's rank correlation coefficient shows strong correlations between patient's {ARAT} scores and movement time as well as movement smoothness. Weak to moderate correlations were observed for parameters that describe hand trajectory similarity and trunk stability. Muscle activity correlates well with grasping activity and the level of grasping force in all groups.",
        "author": "Repnik, Eva and Puh, Ur\u0161ka and Goljar, Nika and Munih, Marko and Mihelj, Matja\u017e",
        "doi": "10.3390/s18092767",
        "file": "Full Text:C\\:\\\\Users\\\\User\\\\Zotero\\\\storage\\\\568Z7HUM\\\\Repnik et al. - 2018 - Using Inertial Measurement Units and Electromyogra.pdf:application/pdf",
        "issn": "1424-8220",
        "journaltitle": "Sensors (Basel, Switzerland)",
        "keywords": "Electromyography, Humans, Adult, Aged, {ARAT Arm, Biomechanical Phenomena, Case-Control Studies, electromyography, Female, inertial measurement unit, Male, Middle Aged, Movement, quantification, stroke, Stroke, upper-limb movement",
        "number": "9",
        "pages": "2767",
        "pmcid": "PMC6164634",
        "pmid": "30135413",
        "series": "Sensors (Basel)",
        "shortjournal": "Sensors (Basel)",
        "title": "Using Inertial Measurement Units and Electromyography to Quantify Movement during Action Research Arm Test Execution",
        "type": "article journal article",
        "volume": "18",
        "year": "2018-08-22"
    },
    "zhang_walk_2025": {
        "abstract": "Robots are becoming more capable and can autonomously perform tasks such as navigating between locations. However, human oversight remains crucial. This study compared two touchless methods for directing mobile robots: voice control and gesture control, to investigate the efficiency of these methods and the preference of users. We tested these methods in two conditions: one in which participants remained stationary and one in which they walked freely alongside the robot. We hypothesized that walking alongside the robot would result in higher intuitiveness ratings and improved task performance, based on the idea that walking promotes spatial alignment and reduces the effort required for mental rotation. In a 2\u00d72 within-subject design, 218 participants guided the quadruped robot Spot along a circuitous route with multiple 90\u00b0 turns using rotate left, rotate right, and walk forward commands. After each trial, participants rated the intuitiveness of the command mapping, while post-experiment interviews were used to gather the participants\u2019 preferences. Results showed that voice control combined with walking with Spot was the most favored and intuitive, whereas gesture control while standing caused confusion for left/right commands. Nevertheless, 29\\% of participants preferred gesture control, citing increased task engagement and visual congruence as reasons. An odometry-based analysis revealed that participants often followed behind Spot, particularly in the gesture control condition, when they were allowed to walk. In conclusion, voice control with walking produced the best outcomes. Improving physical ergonomics and adjusting gesture types could make gesture control more effective.",
        "author": "Zhang, Renchi and Van Der Linden, Jesse and Dodou, Dimitra and Seyffert, Harleigh and Eisma, Yke Bauke and De Winter, Joost",
        "doi": "10.1145/3729540",
        "issn": "2573-9522",
        "journaltitle": "{ACM} Transactions on Human-Robot Interaction",
        "langid": "english",
        "pages": "3729540",
        "series": "ACM Transactions on Human-Robot Interaction",
        "shortjournal": "J. Hum.-Robot Interact.",
        "shorttitle": "Walk Along",
        "title": "Walk Along: An Experiment on Controlling the Mobile Robot \u2018Spot\u2019 with Voice and Gestures",
        "type": "article journal article",
        "url": "https://dl.acm.org/doi/10.1145/3729540",
        "urldate": "2025-05-15",
        "year": "2025-04-12"
    }
}});